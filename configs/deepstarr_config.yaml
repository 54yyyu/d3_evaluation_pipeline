# Configuration for DeepSTARR dataset evaluation
# This extends the base configuration with DeepSTARR-specific settings

# Import base configuration
_base_: "base_config.yaml"

# DeepSTARR-specific data paths
data:
  samples_path: "samples.npz"
  dataset_path: "DeepSTARR_data.h5"
  oracle_path: "oracle_DeepSTARR_DeepSTARR_data.ckpt"
  motif_database_path: "JASPAR2024_CORE_non-redundant_pfms_meme.txt"

# DeepSTARR model configuration
model:
  type: "deepstarr"
  embedding_layer: "model.batchnorm6"
  input_shape: [249, 4]  # [sequence_length, alphabet_size]
  output_dim: 2  # Number of output tasks

# DeepSTARR-specific evaluation settings
evaluation:
  # Use all evaluation types
  run_functional_similarity: true
  run_sequence_similarity: true
  run_compositional_similarity: true
  run_discriminability_similarity: true
  
  # Optimized batch sizes for DeepSTARR sequences
  batch_size: 4096  # For sequence identity calculations
  prediction_batch_size: 128  # For model predictions
  
  # K-mer analysis (3-mers work well for 249bp sequences)
  kmer_lengths: [3, 4]
  
  # Attribution analysis (limit for computational efficiency)
  attribution_max_samples: 2000
  
  # Functional similarity specific settings
  functional_similarity:
    use_both_tasks: true  # Use both enhancer and silencer tasks
    
  # Sequence similarity specific settings  
  sequence_similarity:
    include_train_comparison: true  # Compare synthetic vs training sequences
    
  # Compositional similarity specific settings
  compositional_similarity:
    motif_analysis_method: "auto"  # "auto", "tangermeme", or "fimo"
    attribution_method: "gradient_shap"
    attribution_k_smoothing: 6

# DeepSTARR-specific discriminability settings
discriminability:
  validation_split: 0.2  # Use 20% for validation
  batch_size: 128        # Batch size for DeepSTARR-sized sequences
  train_max_epochs: 50   # Sufficient for binary classification
  patience: 10           # Early stopping patience
  lr: 0.002             # Learning rate (same as oracle model)
  random_seed: 42       # Reproducible data splitting
  save_training_data: true  # Save for potential future reuse
  training_data_path: "results/deepstarr/discriminability_training_data.h5"

# Output settings
output:
  results_dir: "results/deepstarr"
  format: "pickle"
  save_intermediate: false  # Don't save intermediate FASTA files

# Dataset-specific metadata
metadata:
  dataset_name: "DeepSTARR"
  sequence_type: "enhancer_silencer"
  sequence_length: 249
  alphabet_size: 4
  organism: "Drosophila melanogaster"
  tasks: ["dev_enhancer", "hk_enhancer"]